{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXH8xdPjPQ3o"
      },
      "source": [
        "Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1gYFDh8pPKXo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # Build and train ML models\n",
        "from tensorflow.keras import layers, models # Construct models and neural networks\n",
        "import numpy as np # Scientific computing; numerical operations and arrays\n",
        "import random # Random sampling or shuffling\n",
        "import math # Optimization and transformation calculations\n",
        "import matplotlib.pyplot as plt # Data Visualization\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpydZSQDQO1i"
      },
      "source": [
        "Load and Preprocess MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z69GuyWjQUCr"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset and split into training and test sets\n",
        "(train_images, train_labels), (test_images, test_labels), = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape training images and normalize pixel values\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype(\"float32\") / 255\n",
        "\n",
        "# Reshape test images and normalize pixel values\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype(\"float32\") / 255\n",
        "\n",
        "# Convert training labels to one-hot encoded format\n",
        "train_labels_cat = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "\n",
        "# Convert test labels to one-hot encoded format\n",
        "test_labels_cat = tf.keras.utils.to_categorical(test_labels, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU9JWk7vRnzk"
      },
      "source": [
        "Build and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mbxl6jnRygv",
        "outputId": "d90df042-4e3c-4b1c-b2b2-60726cc58b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8568 - loss: 0.4660 - val_accuracy: 0.9780 - val_loss: 0.0816\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1a296ac34d0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simple CNN Model\n",
        "model = models.Sequential([\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation\n",
        "    layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = [28, 28, 1]),\n",
        "    layers.MaxPooling2D((2, 2)), # Max pooling layer with 2x2 pool size\n",
        "    layers.Flatten(), # Flatten the output from the previous layer\n",
        "    layers.Dense(64, activation = \"relu\"), # Fully connected layer with 64 units and ReLU activation\n",
        "    layers.Dense(10, activation = \"softmax\") # Output layer with 10 units (one for each class) and softmax activation\n",
        "])\n",
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "# Train the model for 1 epoch with a batch size of 64 and 10% validation split\n",
        "model.fit(train_images, train_labels_cat, epochs = 1, batch_size = 64, validation_split = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViSQZMHTBLU"
      },
      "source": [
        "Reinforcement Learning Environment Component Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ju6j0_WJTItp"
      },
      "outputs": [],
      "source": [
        "# Rewards matrix and Q-table for RL\n",
        "R = np.array([[0, 1], [1, 0]])\n",
        "Q = np.zeros_like(R, dtype = float)\n",
        "gamma = 0.8 # Discount factor for future rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvFVd_8oTmsf"
      },
      "source": [
        "CRLMM-inspired DL/RL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMZRuacCaYAf"
      },
      "source": [
        "PE07 Update: track correct / incorrect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JkiTMtgOTtMP"
      },
      "outputs": [],
      "source": [
        "# Logistic sigmoid function for CRLMM-inspired adjustment\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Predict function + CRLMM update\n",
        "def predict_and_reward(image, true_label):\n",
        "  pred_probs = model.predict(image[np.newaxis, ...]) [0] # Predict probabilities for each class\n",
        "  action = np.argmax(pred_probs) # Choose action with highest probability\n",
        "  reward = 1 if action == true_label else -1 # 1 for correct prediction, -1 for incorrect\n",
        "  Q[0, action % 2] += gamma * reward # Update Q-table with reward\n",
        "\n",
        "\n",
        "  # Output the action, true label, reward, and Q-table\n",
        "  return action, true_label, reward, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y61wXR01ZoAG"
      },
      "source": [
        "### PE07 Revisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u21heuk7boXR"
      },
      "source": [
        "Tracking Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "coYva2YFbsUt"
      },
      "outputs": [],
      "source": [
        "correct_preds = 0\n",
        "incorrect_preds = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9bTebAEU41Q"
      },
      "source": [
        "Simulation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIo-24f8U-w6",
        "outputId": "139ed58a-66f9-4d2f-f4e2-fe9f894e67b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[0.  0.8]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[0.  1.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[0.8 1.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[1.6 1.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[1.6 2.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[2.4 2.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[3.2 2.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[3.2 3.2]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[3.2 4. ]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[4. 4.]\n",
            " [0. 0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[4.8 4. ]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[4.8 4.8]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[5.6 4.8]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[5.6 5.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[5.6 6.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[6.4 6.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[7.2 6.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[8.  6.4]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[8.  7.2]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[8. 8.]\n",
            " [0. 0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[8.  8.8]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[8.  9.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[8.8 9.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[9.6 9.6]\n",
            " [0.  0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[ 9.6 10.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[10.4 10.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 4, True: 4, Reward: 1, Q-table: [[11.2 10.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[11.2 11.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[12.  11.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[12. 12.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[12.8 12. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[12.8 12.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[12.8 13.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[13.6 13.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[14.4 13.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[14.4 14.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[14.4 15.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[15.2 15.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[15.2 16. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[16. 16.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[16.8 16. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 0, True: 0, Reward: 1, Q-table: [[17.6 16. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[17.6 16.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[17.6 17.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[18.4 17.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 0, True: 0, Reward: 1, Q-table: [[19.2 17.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[19.2 18.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[20.  18.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[20.8 18.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[20.8 19.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[20.8 20. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[20.8 20.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[20.8 21.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[20.8 22.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[20.8 23.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[21.6 23.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[22.4 23.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[22.4 24. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 4, True: 4, Reward: 1, Q-table: [[23.2 24. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[24. 24.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[24.8 24. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[24.8 24.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[24.8 25.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[25.6 25.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[26.4 25.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[27.2 25.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[27.2 26.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[28.  26.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[28.  27.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[28. 28.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[28.  28.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[28.8 28.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 2, True: 2, Reward: 1, Q-table: [[29.6 28.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 4, True: 9, Reward: -1, Q-table: [[28.8 28.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[28.8 29.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[28.8 30.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 0, True: 0, Reward: 1, Q-table: [[29.6 30.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[29.6 31.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 7, True: 7, Reward: 1, Q-table: [[29.6 32. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Predicted: 4, True: 4, Reward: 1, Q-table: [[30.4 32. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 4, True: 4, Reward: 1, Q-table: [[31.2 32. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[32. 32.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[32.  32.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[32.  33.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[32.  34.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[32.  35.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[32. 36.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[32.  36.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 3, True: 7, Reward: -1, Q-table: [[32. 36.]\n",
            " [ 0.  0.]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[32.  36.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[32.  37.6]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 1, True: 1, Reward: 1, Q-table: [[32.  38.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[32.8 38.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 6, True: 6, Reward: 1, Q-table: [[33.6 38.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 8, True: 8, Reward: 1, Q-table: [[34.4 38.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 4, True: 4, Reward: 1, Q-table: [[35.2 38.4]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[35.2 39.2]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 9, True: 9, Reward: 1, Q-table: [[35.2 40. ]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Predicted: 3, True: 3, Reward: 1, Q-table: [[35.2 40.8]\n",
            " [ 0.   0. ]]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Predicted: 5, True: 5, Reward: 1, Q-table: [[35.2 41.6]\n",
            " [ 0.   0. ]]\n",
            "Correct 98, Incorrect: 2, Accuracy 98.00%\n"
          ]
        }
      ],
      "source": [
        "# Run simulation\n",
        "for i in range(100):\n",
        "  # Randomly select an index from the test set\n",
        "  idx = random.randint(0, len(test_images) - 1)\n",
        "  # Predict and update Q-table\n",
        "  action, true_label, reward, Q = predict_and_reward(test_images[idx], test_labels[idx])\n",
        "  # Output Results\n",
        "  print(f\"Predicted: {action}, True: {true_label}, Reward: {reward}, Q-table: {Q}\")\n",
        "  # Count correctness\n",
        "  correct_preds += 1 if reward == 1 else 0\n",
        "  incorrect_preds += 1 if reward == -1 else 0\n",
        "\n",
        "total_preds = correct_preds + incorrect_preds\n",
        "\n",
        "print(f\"Correct {correct_preds}, Incorrect: {incorrect_preds}, Accuracy {(correct_preds / total_preds):.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
